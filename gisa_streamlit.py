import streamlit as st
import pandas as pd
import datetime
import time
import openai  # í•„ìš”ì‹œ ë‹¤ë¥¸ API ì„¤ì •ì—ë„ í™œìš© ê°€ëŠ¥

# ìƒˆ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from langchain_chroma import Chroma
from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser

# ê¸°ì¡´ í…ìŠ¤íŠ¸ ë¶„í•  ë„êµ¬ (í•„ìš”ì‹œ ê³„ì† ì‚¬ìš©)
from langchain.text_splitter import CharacterTextSplitter

# OpenAI API í‚¤ ì„¤ì • (ë‹¤ë¥¸ API í‚¤ê°€ í•„ìš”í•œ ê²½ìš° ë³€ê²½)
openai.api_key = st.secrets["OPENAI_API_KEY"]

GITHUB_CSV_URL = f"https://raw.githubusercontent.com/lyh9003/yong/main/Total_Filtered_No_Comment.csv?nocache={int(time.time())}"

def load_data():
    """CSVë¥¼ ë¶ˆëŸ¬ì™€ DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    df = pd.read_csv(GITHUB_CSV_URL, encoding='utf-8-sig')
    df['date'] = pd.to_datetime(df['date'], errors='coerce')
    df = df.sort_values(by='date', ascending=False)
    df['í‚¤ì›Œë“œ_ëª©ë¡'] = df['í‚¤ì›Œë“œ'].apply(lambda x: [k.strip() for k in str(x).split(',')] if pd.notna(x) else [])
    return df

def create_vector_db(df):
    """Chroma ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    df = df.dropna(subset=['title', 'summary'])
    df['content'] = df['title'] + "\n" + df['summary']
    
    # DataFrameì˜ ê° í–‰ì„ Document ê°ì²´ë¡œ ë³€í™˜
    documents = [
        Document(page_content=row['content'], metadata=row.to_dict())
        for idx, row in df.iterrows()
    ]
    
    # í…ìŠ¤íŠ¸ ë¶„í•  (chunking)
    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    split_docs = text_splitter.split_documents(documents)
    
    # OllamaEmbeddingsë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ìƒì„± í›„, Chroma ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•
    embeddings = OllamaEmbeddings()
    vector_db = Chroma.from_documents(split_docs, embeddings)
    return vector_db

def query_rag(vector_db, query):
    """ìƒˆ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ RAG ê¸°ë°˜ ë‰´ìŠ¤ ê¸°ì‚¬ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    # 1. ë²¡í„° ì €ì¥ì†Œì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (ì˜ˆ: ìƒìœ„ 5ê°œ)
    retriever = vector_db.as_retriever(search_kwargs={"k": 5})
    retrieved_docs = retriever.get_relevant_documents(query)
    
    # 2. CrossEncoderë¥¼ ì´ìš©í•´ ë¬¸ì„œ ì¬ì •ë ¬ (HuggingFace ëª¨ë¸ ì‚¬ìš©)
    cross_encoder = HuggingFaceCrossEncoder(model_name="cross-encoder/ms-marco-MiniLM-L-6-v2")
    reranker = CrossEncoderReranker(cross_encoder=cross_encoder)
    reranked_docs = reranker.rerank(query, retrieved_docs)
    
    # 3. ì¬ì •ë ¬ëœ ë¬¸ì„œë“¤ì˜ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ì»¨í…ìŠ¤íŠ¸ë¡œ ê²°í•©
    context = "\n\n".join([doc.page_content for doc in reranked_docs])
    
    # 4. ChatPromptTemplateì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", "You are a helpful assistant that summarizes news articles based on provided context."),
        ("user", "Given the following context:\n{context}\n\nPlease summarize the key points answering: {query}")
    ])
    prompt = prompt_template.format(context=context, query=query)
    
    # 5. ChatOllamaë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„± (ì˜ˆì‹œë¡œ 'llama2' ëª¨ë¸ ì‚¬ìš©)
    llm = ChatOllama(model="llama2", temperature=0.3)
    response = llm(prompt)
    return response

# Streamlit UI ì„¤ì •
df = load_data()
vector_db = create_vector_db(df)

st.title("ğŸ“¢ ë°˜ë„ì²´ ë‰´ìŠ¤ RAG ê¸°ë°˜ ê²€ìƒ‰")
st.write("ë¬¸ì˜/ì•„ì´ë””ì–´ : yh9003.lee@samsung.com")

# ì‚¬ìš©ì ì…ë ¥
query = st.text_input("ğŸ” ê²€ìƒ‰í•  ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”:")
if query:
    with st.spinner("ğŸ” ê²€ìƒ‰ ì¤‘..."):
        response = query_rag(vector_db, query)
    st.markdown("### ğŸ“œ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½")
    st.write(response)

# ê¸°ë³¸ì ì¸ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥
st.write(f"**ì´ ê¸°ì‚¬ ìˆ˜:** {len(df)}ê°œ")
grouped_by_date = df.groupby(df['date'].dt.date, sort=False)
for current_date, date_group in grouped_by_date:
    st.markdown(f"## {current_date.strftime('%Y-%m-%d')}")
    for idx, row in date_group.iterrows():
        with st.expander(f"ğŸ“° {row['title']}"):
            st.write(f"**ìš”ì•½:** {row.get('summary', 'ìš”ì•½ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.')}")
            link = row.get('link', None)
            if pd.notna(link):
                st.markdown(f"[ğŸ”— ê¸°ì‚¬ ë§í¬]({link})")
