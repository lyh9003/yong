import streamlit as st
import pandas as pd
import datetime
import time
import openai
import pysqlite3
import sys
import os
sys.modules['sqlite3'] = pysqlite3  # sqlite3 ëª¨ë“ˆì„ ìµœì‹  ë²„ì „ìœ¼ë¡œ êµì²´

import sqlite3  # ì´ì œ ìµœì‹  sqlite3 ì‚¬ìš© ê°€ëŠ¥
# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.docstore.document import Document

# OpenAI API í‚¤ ì„¤ì • (Streamlit secretsì— ë“±ë¡)
openai.api_key = st.secrets["OPENAI_API_KEY"]

GITHUB_CSV_URL = f"https://raw.githubusercontent.com/lyh9003/yong/main/Total_Filtered_No_Comment.csv?nocache={int(time.time())}"

def load_data():
    """CSVë¥¼ ë¶ˆëŸ¬ì™€ DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    df = pd.read_csv(GITHUB_CSV_URL, encoding='utf-8-sig')
    
    # 1) ë‚ ì§œ ì»¬ëŸ¼ì„ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    df['date'] = pd.to_datetime(df['date'], errors='coerce')
    
    # 2) ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (ê°€ì¥ ìµœê·¼ ê¸°ì‚¬ê°€ ìœ„ë¡œ)
    df = df.sort_values(by='date', ascending=False)
    
    # 3) 'í‚¤ì›Œë“œ' ì»¬ëŸ¼ì„ ì‰¼í‘œ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ë¦¬ìŠ¤íŠ¸í™”
    def split_keywords(kw_string):
        if pd.isna(kw_string):
            return []
        return [k.strip() for k in kw_string.split(',') if k.strip()]
    
    df['í‚¤ì›Œë“œ_ëª©ë¡'] = df['í‚¤ì›Œë“œ'].apply(split_keywords)
    
    # 4) explodeë¥¼ ì´ìš©í•˜ì—¬ í‚¤ì›Œë“œë³„ë¡œ ë ˆì½”ë“œë¥¼ í¼ì¹¨
    df = df.explode('í‚¤ì›Œë“œ_ëª©ë¡', ignore_index=True)
    
    # 5) 'ê´€ë ¨ ì—†ìŒ'ì„ 'ê¸°íƒ€'ë¡œ ë³€ê²½
    df['í‚¤ì›Œë“œ_ëª©ë¡'] = df['í‚¤ì›Œë“œ_ëª©ë¡'].replace('ê´€ë ¨ ì—†ìŒ', 'ê¸°íƒ€')
    
    return df

# ===============================================
# OpenAIë¥¼ í™œìš©í•œ í—¬í¼ í•¨ìˆ˜ë“¤
# ===============================================
def check_semiconductor(question):
    """
    ì§ˆë¬¸ì´ ë°˜ë„ì²´ì™€ ê´€ë ¨ëœì§€ í™•ì¸í•©ë‹ˆë‹¤.
    OpenAI APIì— 'ì˜ˆ' ë˜ëŠ” 'ì•„ë‹ˆì˜¤'ë¡œ ëŒ€ë‹µí•˜ë„ë¡ ìš”ì²­í•©ë‹ˆë‹¤.
    """
    prompt = f"ë‹¤ìŒ ì§ˆë¬¸ì´ ë°˜ë„ì²´ì™€ ê´€ë ¨ì´ ìˆìœ¼ë©´ 'ì˜ˆ', ì•„ë‹ˆë©´ 'ì•„ë‹ˆì˜¤'ë¡œ ëŒ€ë‹µí•´ì¤˜:\n{question}"
    response = openai.Completion.create(
        model="GPT-4o-mini",
        prompt=prompt,
        max_tokens=3,
        temperature=0
    )
    answer = response.choices[0].text.strip()
    return answer == "ì˜ˆ"

def extract_keyword(question):
    """
    ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ í•œ ë‹¨ì–´ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    prompt = f"ë‹¤ìŒ ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ í•œ ë‹¨ì–´ë¡œ ì¶”ì¶œí•´ì¤˜:\n{question}"
    response = openai.Completion.create(
        model="GPT-4o-mini",
        prompt=prompt,
        max_tokens=5,
        temperature=0
    )
    keyword = response.choices[0].text.strip()
    return keyword

def generate_answer_openai(question):
    """
    ì¼ë°˜ì ì¸ ì§ˆë¬¸ì— ëŒ€í•´ OpenAIë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€ ìƒì„±í•©ë‹ˆë‹¤.
    """
    response = openai.Completion.create(
        model="GPT-4o-mini",
        prompt=question,
        max_tokens=150,
        temperature=0.7
    )
    return response.choices[0].text.strip()

def generate_answer_with_rag(question, context):
    """
    ë‰´ìŠ¤ ê¸°ì‚¬ ë‚´ìš©ì„ contextë¡œ í•˜ì—¬ RAG ë°©ì‹ì˜ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    prompt = f"ì£¼ì–´ì§„ ë‰´ìŠ¤ ê¸°ì‚¬ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€ì„ ìƒì„±í•´ì¤˜.\n\në‰´ìŠ¤ ê¸°ì‚¬:\n{context}\n\nì§ˆë¬¸:\n{question}\n\në‹µë³€:"
    response = openai.Completion.create(
        model="GPT-4o-mini",
        prompt=prompt,
        max_tokens=150,
        temperature=0.7
    )
    return response.choices[0].text.strip()

# ===============================================
# ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„°ì…‹ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± (Chroma)
# ===============================================


CHROMA_PERSIST_DIR = "./chroma_db"  # ìƒëŒ€ ê²½ë¡œ ì‚¬ìš©

# ë””ë ‰í„°ë¦¬ ìƒì„±
os.makedirs(CHROMA_PERSIST_DIR, exist_ok=True)



def build_vector_store(df):
    """
    ê° ê¸°ì‚¬ì˜ ì œëª©ê³¼ ìš”ì•½ì„ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ê²°í•©í•œ í›„,
    LangChainì˜ OpenAIEmbeddingsë¥¼ ì´ìš©í•´ Chroma ë²¡í„° ìŠ¤í† ì–´ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.
    """
    documents = []
    for idx, row in df.iterrows():
        content = f"ì œëª©: {row['title']}\nìš”ì•½: {row.get('summary', 'ìš”ì•½ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.')}"
        documents.append(Document(page_content=content, metadata={"í‚¤ì›Œë“œ": row["í‚¤ì›Œë“œ_ëª©ë¡"], "date": row["date"]}))
    embeddings = OpenAIEmbeddings()
    vector_store = Chroma.from_documents(
        documents,
        embeddings,
        collection_name="news_articles",
        persist_directory=CHROMA_PERSIST_DIR  # ì¶”ê°€ ì„¤ì •
    )
    return vector_store

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df = load_data()

# ìµœê·¼ 1ì£¼ì¼ì¹˜ ê¸°ì‚¬ í•„í„°ë§ìš© ë°ì´í„° ì¤€ë¹„
if not df.empty:
    max_date = df['date'].max()
    one_week_ago = max_date - datetime.timedelta(days=7)
    default_recent_df = df[df['date'] >= one_week_ago]
else:
    default_recent_df = df.copy()

# ë²¡í„° ìŠ¤í† ì–´ ìƒì„± (RAG ê²€ìƒ‰ì— í™œìš©)
if not df.empty:
    vector_store = build_vector_store(df)

# ======================================================
# Streamlit ì•± íƒ€ì´í‹€ ë° ê¸°ë³¸ ì„¤ì •
# ======================================================
st.title("ğŸ“¢ ë°˜ë„ì²´ ë‰´ìŠ¤ ì—…ë°ì´íŠ¸")
st.write("yh9003.lee@samsung.com")

# ======================================================
# [ì¶”ê°€] ì§ˆë¬¸ ì…ë ¥ë€ ë° RAG/ì¼ë°˜ ê²€ìƒ‰ ë¶„ê¸° ì²˜ë¦¬
# ======================================================
st.header("â“ ì§ˆë¬¸ ì…ë ¥")
question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")

if question:
    if check_semiconductor(question):
        st.write("**ë°˜ë„ì²´ ê´€ë ¨ ì§ˆë¬¸ìœ¼ë¡œ ì¸ì‹í•˜ì˜€ìŠµë‹ˆë‹¤.**")
        # ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        keyword = extract_keyword(question)
        st.write(f"ì¶”ì¶œëœ í‚¤ì›Œë“œ: **{keyword}**")
        # ë²¡í„° ìŠ¤í† ì–´ë¥¼ í™œìš©í•˜ì—¬ í•´ë‹¹ í‚¤ì›Œë“œì™€ ê°€ì¥ ë°€ì ‘í•œ ê¸°ì‚¬ ê²€ìƒ‰
        docs = vector_store.similarity_search(keyword, k=1)
        if docs:
            context = docs[0].page_content
            st.write("**ê²€ìƒ‰ëœ ë‰´ìŠ¤ ê¸°ì‚¬:**")
            st.write(context)
            # RAG ë°©ì‹ìœ¼ë¡œ ë‹µë³€ ìƒì„±
            answer = generate_answer_with_rag(question, context)
            st.write("**RAGë¥¼ í†µí•œ ë‹µë³€:**")
            st.write(answer)
        else:
            st.write("í•´ë‹¹ í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.write("**ë°˜ë„ì²´ ê´€ë ¨ ì§ˆë¬¸ì´ ì•„ë‹ˆë¼ê³  íŒë‹¨ë˜ì–´ ì¼ë°˜ OpenAI ê²€ìƒ‰ì„ ì§„í–‰í•©ë‹ˆë‹¤.**")
        answer = generate_answer_openai(question)
        st.write("**ë‹µë³€:**")
        st.write(answer)

# ======================================================
# 1) ì‚¬ì´ë“œë°” í•„í„° (ë‚ ì§œ ì„ íƒ)
# ======================================================
unique_dates = sorted(list(set(df['date'].dt.date.dropna())), reverse=True)

selected_dates = st.sidebar.multiselect(
    "ğŸ“… ë‚ ì§œë¥¼ ì„ íƒí•˜ì„¸ìš” (ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)",
    unique_dates,
    help="ì•„ë¬´ ê²ƒë„ ì„ íƒí•˜ì§€ ì•Šìœ¼ë©´ ìµœê·¼ 1ì£¼ì¼ì¹˜ ê¸°ì‚¬ê°€ í‘œì‹œë©ë‹ˆë‹¤."
)

# ======================================================
# 2) í‚¤ì›Œë“œ í•„í„° ì¶”ê°€ (ì¹´í…Œê³ ë¦¬ ì—­í• , 'ê´€ë ¨ ì—†ìŒ' â†’ 'ê¸°íƒ€')
# ======================================================
unique_keywords = sorted(list(df['í‚¤ì›Œë“œ_ëª©ë¡'].dropna().unique()))

selected_keywords = st.sidebar.multiselect(
    "ğŸ” í‚¤ì›Œë“œë¥¼ ì„ íƒí•˜ì„¸ìš” (ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)",
    unique_keywords,
    help="ì•„ë¬´ ê²ƒë„ ì„ íƒí•˜ì§€ ì•Šìœ¼ë©´ ëª¨ë“  í‚¤ì›Œë“œê°€ í‘œì‹œë©ë‹ˆë‹¤."
)

# ======================================================
# 3) í•„í„° ì ìš© (ë‚ ì§œ + í‚¤ì›Œë“œ)
# ======================================================
filtered_df = df.copy()

# ë‚ ì§œ í•„í„° ì ìš©
if selected_dates:
    filtered_df = filtered_df[filtered_df['date'].dt.date.isin(selected_dates)]
else:
    filtered_df = default_recent_df

# í‚¤ì›Œë“œ í•„í„° ì ìš©
if selected_keywords:
    filtered_df = filtered_df[filtered_df['í‚¤ì›Œë“œ_ëª©ë¡'].isin(selected_keywords)]

st.write(f"**ì´ ê¸°ì‚¬ ìˆ˜:** {len(filtered_df)}ê°œ")

# ======================================================
# 4) ë‚ ì§œë³„ â†’ í‚¤ì›Œë“œë³„ â†’ ê¸°ì‚¬ ëª©ë¡ í‘œì‹œ
# ======================================================
grouped_by_date = filtered_df.groupby(filtered_df['date'].dt.date, sort=False)

for current_date, date_group in grouped_by_date:
    st.markdown(f"## {current_date.strftime('%Y-%m-%d')}")
    grouped_by_keyword = date_group.groupby('í‚¤ì›Œë“œ_ëª©ë¡', sort=False)
    
    for keyword_value, keyword_group in grouped_by_keyword:
        if pd.notna(keyword_value) and str(keyword_value).strip():
            st.markdown(f"### â–¶ï¸ {keyword_value}")
        else:
            st.markdown("### â–¶ï¸ (í‚¤ì›Œë“œ ì—†ìŒ)")
        
        for idx, row in keyword_group.iterrows():
            # ì œëª©ì„ ë²„íŠ¼ìœ¼ë¡œ ë§Œë“¤ì–´ í´ë¦­ ì‹œ ìš”ì•½ì´ í‘œì‹œë˜ë„ë¡ í•¨
            if st.button(f"ğŸ“° {row['title']}", key=f"title_{idx}"):
                st.write(f"**ìš”ì•½:** {row.get('summary', 'ìš”ì•½ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.')}")
                link = row.get('link', None)
                if pd.notna(link):
                    st.markdown(f"[ğŸ”— ê¸°ì‚¬ ë§í¬]({link})")
                else:
                    st.write("ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
