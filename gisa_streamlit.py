import streamlit as st
import pandas as pd
import datetime
import time

# RAG ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter

GITHUB_CSV_URL = f"https://raw.githubusercontent.com/lyh9003/yong/main/Total_Filtered_No_Comment.csv?nocache={int(time.time())}"

def load_data():
    """CSVë¥¼ ë¶ˆëŸ¬ì™€ DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    df = pd.read_csv(GITHUB_CSV_URL, encoding='utf-8-sig')
    
    # 1) ë‚ ì§œ ì»¬ëŸ¼ì„ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    df['date'] = pd.to_datetime(df['date'], errors='coerce')
    
    # 2) ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (ê°€ì¥ ìµœê·¼ ê¸°ì‚¬ê°€ ìœ„ë¡œ)
    df = df.sort_values(by='date', ascending=False)
    
    # 3) 'í‚¤ì›Œë“œ' ì»¬ëŸ¼ì„ ì‰¼í‘œ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ë¦¬ìŠ¤íŠ¸í™”
    def split_keywords(kw_string):
        if pd.isna(kw_string):
            return []
        return [k.strip() for k in kw_string.split(',') if k.strip()]
    
    df['í‚¤ì›Œë“œ_ëª©ë¡'] = df['í‚¤ì›Œë“œ'].apply(split_keywords)
    
    # 4) explodeë¥¼ ì´ìš©í•˜ì—¬ í‚¤ì›Œë“œë³„ë¡œ ë ˆì½”ë“œë¥¼ í¼ì¹¨
    df = df.explode('í‚¤ì›Œë“œ_ëª©ë¡', ignore_index=True)
    
    # 5) 'ê´€ë ¨ ì—†ìŒ'ì„ 'ê¸°íƒ€'ë¡œ ë³€ê²½
    df['í‚¤ì›Œë“œ_ëª©ë¡'] = df['í‚¤ì›Œë“œ_ëª©ë¡'].replace('ê´€ë ¨ ì—†ìŒ', 'ê¸°íƒ€')
    
    return df

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df = load_data()

# =============================================================================
# RAGë¥¼ ìœ„í•œ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± í•¨ìˆ˜ (ìºì‹± ì²˜ë¦¬)
# =============================================================================
@st.cache_resource
def create_vector_store(dataframe):
    # ê° ê¸°ì‚¬ì—ì„œ ì œëª©ê³¼ ìš”ì•½ì„ ê²°í•©í•˜ì—¬ í…ìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    texts = []
    for _, row in dataframe.iterrows():
        title = row.get('title', '')
        summary = row.get('summary', '')
        combined_text = f"ì œëª©: {title}\nìš”ì•½: {summary}"
        texts.append(combined_text)
    
    # ë¬¸ì„œì˜ ê¸¸ì´ê°€ ê¸¸ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ í…ìŠ¤íŠ¸ ë¶„í• ê¸°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    split_texts = []
    for text in texts:
        split_texts.extend(text_splitter.split_text(text))
    
    # OpenAI ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ Chroma ë²¡í„° ìŠ¤í† ì–´ ìƒì„± (persist_directoryë¥¼ Noneìœ¼ë¡œ í•˜ì—¬ ë©”ëª¨ë¦¬ ë‚´ ì €ì¥)
    embeddings = OpenAIEmbeddings()
    vector_store = Chroma.from_texts(split_texts, embedding=embeddings, persist_directory=None)
    return vector_store

# =============================================================================
# ë‚ ì§œ í•„í„°ë§ì„ ìœ„í•œ ê¸°ë³¸ ë°ì´í„° ì¤€ë¹„
# =============================================================================
if not df.empty:
    max_date = df['date'].max()
    one_week_ago = max_date - datetime.timedelta(days=7)
    one_month_ago = max_date - datetime.timedelta(days=30)
else:
    one_week_ago = one_month_ago = None

# =============================================================================
# Streamlit ì•± íƒ€ì´í‹€
# =============================================================================
st.title("ğŸ“¢ë°˜ë„ì²´ ë‰´ìŠ¤ë ˆí„°(Rev.25.3.13)")
st.write("ë¬¸ì˜/ì•„ì´ë””ì–´ : yh9003.lee@samsung.com")

# =============================================================================
# ì‚¬ì´ë“œë°” í•„í„° ì˜µì…˜ ì„¤ì • (ë‚ ì§œ, í‚¤ì›Œë“œ, ê²€ìƒ‰ì–´)
# =============================================================================
date_filter_option = st.sidebar.radio(
    "ğŸ“… ë‚ ì§œ í•„í„° ì˜µì…˜",
    ["ìµœê·¼ 7ì¼", "ìµœê·¼ 1ë‹¬", "ì „ì²´", "ì§ì ‘ ì„ íƒ"],
    index=0
)

unique_dates = sorted(list(set(df['date'].dt.date.dropna())), reverse=True)

if date_filter_option == "ìµœê·¼ 7ì¼":
    selected_dates = [date for date in unique_dates if date >= one_week_ago.date()]
elif date_filter_option == "ìµœê·¼ 1ë‹¬":
    selected_dates = [date for date in unique_dates if date >= one_month_ago.date()]
elif date_filter_option == "ì „ì²´":
    selected_dates = unique_dates
else:  # "ì§ì ‘ ì„ íƒ"
    selected_dates = st.sidebar.multiselect(
        "ğŸ“… ë‚ ì§œë¥¼ ì„ íƒí•˜ì„¸ìš” (ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)",
        unique_dates,
        help="í•„í„° ì˜µì…˜ì—ì„œ 'ì§ì ‘ ì„ íƒ'ì„ ì„ íƒí•œ ê²½ìš°ì—ë§Œ í™œì„±í™”ë©ë‹ˆë‹¤."
    )

unique_keywords = sorted(list(df['í‚¤ì›Œë“œ_ëª©ë¡'].dropna().unique()))
selected_keywords = st.sidebar.multiselect(
    "ğŸ” í‚¤ì›Œë“œë¥¼ ì„ íƒí•˜ì„¸ìš” (ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)",
    unique_keywords,
    help="ì•„ë¬´ ê²ƒë„ ì„ íƒí•˜ì§€ ì•Šìœ¼ë©´ ëª¨ë“  í‚¤ì›Œë“œê°€ í‘œì‹œë©ë‹ˆë‹¤."
)

search_query = st.sidebar.text_input(
    "ğŸ” ê²€ìƒ‰ì–´ ì…ë ¥ (ì œëª©/ìš”ì•½ í¬í•¨)",
    help="íŠ¹ì • ë‹¨ì–´ê°€ í¬í•¨ëœ ê¸°ì‚¬ë§Œ ê²€ìƒ‰í•©ë‹ˆë‹¤."
)

# =============================================================================
# í•„í„° ì ìš© (ë‚ ì§œ + í‚¤ì›Œë“œ + ê²€ìƒ‰ì–´)
# =============================================================================
filtered_df = df.copy()

if selected_dates:
    filtered_df = filtered_df[filtered_df['date'].dt.date.isin(selected_dates)]

if selected_keywords:
    filtered_df = filtered_df[filtered_df['í‚¤ì›Œë“œ_ëª©ë¡'].isin(selected_keywords)]

if search_query:
    search_query_lower = search_query.lower()
    filtered_df = filtered_df[
        filtered_df['title'].str.lower().str.contains(search_query_lower, na=False) |
        filtered_df['summary'].fillna('').str.lower().str.contains(search_query_lower, na=False)
    ]

st.write(f"**ì´ ê¸°ì‚¬ ìˆ˜:** {len(filtered_df)}ê°œ")

# =============================================================================
# ë‚ ì§œë³„ â†’ í‚¤ì›Œë“œë³„ â†’ ê¸°ì‚¬ ëª©ë¡ í‘œì‹œ (ì œëª© í´ë¦­ ì‹œ ìš”ì•½ & ë§í¬ í‘œì‹œ)
# =============================================================================
grouped_by_date = filtered_df.groupby(filtered_df['date'].dt.date, sort=False)

for current_date, date_group in grouped_by_date:
    st.markdown(f"## {current_date.strftime('%Y-%m-%d')}")
    grouped_by_keyword = date_group.groupby('í‚¤ì›Œë“œ_ëª©ë¡', sort=False)
    
    for keyword_value, keyword_group in grouped_by_keyword:
        if pd.notna(keyword_value) and str(keyword_value).strip():
            st.markdown(f"### â–¶ï¸ {keyword_value}")
        else:
            st.markdown("### â–¶ï¸ (í‚¤ì›Œë“œ ì—†ìŒ)")
        
        for idx, row in keyword_group.iterrows():
            with st.expander(f"ğŸ“° {row['title']}"):
                st.write(f"**ìš”ì•½:** {row.get('summary', 'ìš”ì•½ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.')}")
                link = row.get('link', None)
                if pd.notna(link):
                    st.markdown(f"[ğŸ”— ê¸°ì‚¬ ë§í¬]({link})")
                else:
                    st.write("ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

# =============================================================================
# RAG (Retrieval-Augmented Generation) ê¸°ëŠ¥ ì¶”ê°€
# =============================================================================
st.markdown("## ğŸ¤– RAG ì§ˆë¬¸í•˜ê¸°")
rag_question = st.text_input("ê¸°ì‚¬ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", key="rag_question")

if rag_question:
    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        # ì „ì²´ ê¸°ì‚¬ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± (ìºì‹±ë˜ì–´ ìˆìŒ)
        vector_store = create_vector_store(df)
        
        # ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ë¬¸ì„œ ì¡°ê° 3ê°œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        retrieved_docs = vector_store.similarity_search(rag_question, k=3)
        context = "\n\n".join([doc.page_content for doc in retrieved_docs])
        
        # ChatGPTì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ë¬¸ë§¥ ì •ë³´ + ì§ˆë¬¸)
        prompt = f"ë‹¤ìŒ ê¸°ì‚¬ ì •ë³´ë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:\n\n{context}\n\nì§ˆë¬¸: {rag_question}\në‹µë³€:"
        
        chat = ChatOpenAI(temperature=0.7)
        response = chat([{"role": "user", "content": prompt}])
        answer = response.content
        
    st.markdown("### ë‹µë³€")
    st.write(answer)
