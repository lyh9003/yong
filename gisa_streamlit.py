import streamlit as st
import pandas as pd
import datetime
import time

# RAG ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import Ollama

# CSV íŒŒì¼ URL (GitHub)
GITHUB_CSV_URL = f"https://raw.githubusercontent.com/lyh9003/yong/main/Total_Filtered_No_Comment.csv?nocache={int(time.time())}"

def load_data():
    """CSVë¥¼ ë¶ˆëŸ¬ì™€ DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    df = pd.read_csv(GITHUB_CSV_URL, encoding='utf-8-sig')
    
    # 1) ë‚ ì§œ ì»¬ëŸ¼ì„ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    df['date'] = pd.to_datetime(df['date'], errors='coerce')
    
    # 2) ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (ê°€ì¥ ìµœê·¼ ê¸°ì‚¬ê°€ ìœ„ë¡œ)
    df = df.sort_values(by='date', ascending=False)
    
    # 3) 'í‚¤ì›Œë“œ' ì»¬ëŸ¼ì„ ì‰¼í‘œ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ë¦¬ìŠ¤íŠ¸í™”
    def split_keywords(kw_string):
        if pd.isna(kw_string):
            return []
        return [k.strip() for k in kw_string.split(',') if k.strip()]
    
    df['í‚¤ì›Œë“œ_ëª©ë¡'] = df['í‚¤ì›Œë“œ'].apply(split_keywords)
    
    # 4) explodeë¥¼ ì´ìš©í•˜ì—¬ í‚¤ì›Œë“œë³„ë¡œ ë ˆì½”ë“œë¥¼ í¼ì¹¨
    df = df.explode('í‚¤ì›Œë“œ_ëª©ë¡', ignore_index=True)
    
    # 5) 'ê´€ë ¨ ì—†ìŒ'ì„ 'ê¸°íƒ€'ë¡œ ë³€ê²½
    df['í‚¤ì›Œë“œ_ëª©ë¡'] = df['í‚¤ì›Œë“œ_ëª©ë¡'].replace('ê´€ë ¨ ì—†ìŒ', 'ê¸°íƒ€')
    
    return df

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df = load_data()

# ======================================================
# ë‚ ì§œ í•„í„°ë§ì„ ìœ„í•œ ê¸°ë³¸ ë°ì´í„° ì¤€ë¹„
# ======================================================
if not df.empty:
    max_date = df['date'].max()
    one_week_ago = max_date - datetime.timedelta(days=7)
    one_month_ago = max_date - datetime.timedelta(days=30)
else:
    one_week_ago = one_month_ago = None

# ======================================================
# Streamlit ì•± íƒ€ì´í‹€
# ======================================================
st.title("ğŸ“¢ë°˜ë„ì²´ ë‰´ìŠ¤ë ˆí„°(Rev.25.3.13)")
st.write("ë¬¸ì˜/ì•„ì´ë””ì–´ : yh9003.lee@samsung.com")

# ======================================================
# ì‚¬ì´ë“œë°” ë‚ ì§œ í•„í„° ì˜µì…˜ ì¶”ê°€
# ======================================================
date_filter_option = st.sidebar.radio(
    "ğŸ“… ë‚ ì§œ í•„í„° ì˜µì…˜",
    ["ìµœê·¼ 7ì¼", "ìµœê·¼ 1ë‹¬", "ì „ì²´", "ì§ì ‘ ì„ íƒ"],
    index=0
)

unique_dates = sorted(list(set(df['date'].dt.date.dropna())), reverse=True)

# ë‚ ì§œ í•„í„° ì˜µì…˜ ì ìš©
if date_filter_option == "ìµœê·¼ 7ì¼":
    selected_dates = [date for date in unique_dates if date >= one_week_ago.date()]
elif date_filter_option == "ìµœê·¼ 1ë‹¬":
    selected_dates = [date for date in unique_dates if date >= one_month_ago.date()]
elif date_filter_option == "ì „ì²´":
    selected_dates = unique_dates  # ëª¨ë“  ë‚ ì§œ í¬í•¨
else:  # "ì§ì ‘ ì„ íƒ"
    selected_dates = st.sidebar.multiselect(
        "ğŸ“… ë‚ ì§œë¥¼ ì„ íƒí•˜ì„¸ìš” (ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)",
        unique_dates,
        help="í•„í„° ì˜µì…˜ì—ì„œ 'ì§ì ‘ ì„ íƒ'ì„ ì„ íƒí•œ ê²½ìš°ì—ë§Œ í™œì„±í™”ë©ë‹ˆë‹¤."
    )

# ======================================================
# í‚¤ì›Œë“œ í•„í„° ì¶”ê°€ (ì¹´í…Œê³ ë¦¬ ì—­í• , 'ê´€ë ¨ ì—†ìŒ' â†’ 'ê¸°íƒ€')
# ======================================================
unique_keywords = sorted(list(df['í‚¤ì›Œë“œ_ëª©ë¡'].dropna().unique()))

selected_keywords = st.sidebar.multiselect(
    "ğŸ” í‚¤ì›Œë“œë¥¼ ì„ íƒí•˜ì„¸ìš” (ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)",
    unique_keywords,
    help="ì•„ë¬´ ê²ƒë„ ì„ íƒí•˜ì§€ ì•Šìœ¼ë©´ ëª¨ë“  í‚¤ì›Œë“œê°€ í‘œì‹œë©ë‹ˆë‹¤."
)

# ======================================================
# ê²€ìƒ‰ì–´ í•„í„° ì¶”ê°€ (ì œëª© ë° ìš”ì•½ ê²€ìƒ‰)
# ======================================================
search_query = st.sidebar.text_input(
    "ğŸ” ê²€ìƒ‰ì–´ ì…ë ¥ (ì œëª©/ìš”ì•½ í¬í•¨)",
    help="íŠ¹ì • ë‹¨ì–´ê°€ í¬í•¨ëœ ê¸°ì‚¬ë§Œ ê²€ìƒ‰í•©ë‹ˆë‹¤."
)

# ======================================================
# í•„í„° ì ìš© (ë‚ ì§œ + í‚¤ì›Œë“œ + ê²€ìƒ‰ì–´)
# ======================================================
filtered_df = df.copy()

# ë‚ ì§œ í•„í„° ì ìš©
if selected_dates:
    filtered_df = filtered_df[filtered_df['date'].dt.date.isin(selected_dates)]

# í‚¤ì›Œë“œ í•„í„° ì ìš©
if selected_keywords:
    filtered_df = filtered_df[filtered_df['í‚¤ì›Œë“œ_ëª©ë¡'].isin(selected_keywords)]

# ê²€ìƒ‰ì–´ í•„í„° ì ìš© (ì œëª© + ìš”ì•½)
if search_query:
    search_query = search_query.lower()
    filtered_df = filtered_df[
        filtered_df['title'].str.lower().str.contains(search_query, na=False) |
        filtered_df['summary'].fillna('').str.lower().str.contains(search_query, na=False)
    ]

st.write(f"**ì´ ê¸°ì‚¬ ìˆ˜:** {len(filtered_df)}ê°œ")

# ======================================================
# ë‚ ì§œë³„ â†’ í‚¤ì›Œë“œë³„ â†’ ê¸°ì‚¬ ëª©ë¡ í‘œì‹œ (ì œëª© í´ë¦­ ì‹œ ìš”ì•½ & ë§í¬ í‘œì‹œ)
# ======================================================
grouped_by_date = filtered_df.groupby(filtered_df['date'].dt.date, sort=False)

for current_date, date_group in grouped_by_date:
    st.markdown(f"## {current_date.strftime('%Y-%m-%d')}")
    grouped_by_keyword = date_group.groupby('í‚¤ì›Œë“œ_ëª©ë¡', sort=False)
    
    for keyword_value, keyword_group in grouped_by_keyword:
        if pd.notna(keyword_value) and str(keyword_value).strip():
            st.markdown(f"### â–¶ï¸ {keyword_value}")
        else:
            st.markdown("### â–¶ï¸ (í‚¤ì›Œë“œ ì—†ìŒ)")
        
        for idx, row in keyword_group.iterrows():
            with st.expander(f"ğŸ“° {row['title']}"):
                st.write(f"**ìš”ì•½:** {row.get('summary', 'ìš”ì•½ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.')}")
                link = row.get('link', None)
                if pd.notna(link):
                    st.markdown(f"[ğŸ”— ê¸°ì‚¬ ë§í¬]({link})")
                else:
                    st.write("ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ======================================================
# RAG(ê²€ìƒ‰ê¸°ë°˜ ìƒì„±) ê¸°ëŠ¥ êµ¬í˜„
# ======================================================

# ìºì‹±ì„ í†µí•´ ë²¡í„°ìŠ¤í† ì–´ ë¹Œë“œ ë¹„ìš© ìµœì†Œí™” (Streamlit 1.18 ì´ìƒ st.cache_resource ì‚¬ìš©)
# RAGìš© ë²¡í„°ìŠ¤í† ì–´ ìƒì„± í•¨ìˆ˜ ë‚´ ìˆ˜ì • ì˜ˆì‹œ
@st.cache_resource
def build_vectorstore(dataframe: pd.DataFrame):
    """
    DataFrameì˜ ê° ê¸°ì‚¬ì—ì„œ ì œëª©ê³¼ ìš”ì•½ì„ ê²°í•©í•œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê³ ,
    SemanticChunkerë¡œ ì²­í¬ë¥¼ ë§Œë“  í›„ FAISS ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    documents = []
    metadatas = []
    for _, row in dataframe.iterrows():
        # ì œëª©ê³¼ ìš”ì•½ì„ ê²°í•© (ìš”ì•½ì´ ì—†ìœ¼ë©´ ì œëª©ë§Œ ì‚¬ìš©)
        text = row.get('title', '')
        summary = row.get('summary', '')
        if pd.notna(summary) and summary.strip():
            text += "\n" + summary
        documents.append(text)
        metadatas.append({
            "date": row.get("date"),
            "í‚¤ì›Œë“œ": row.get("í‚¤ì›Œë“œ_ëª©ë¡")
        })
    
    # LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ chunkerìš©ìœ¼ë¡œ ìƒì„± (ì˜¨ë„ë‚˜ ê¸°íƒ€ íŒŒë¼ë¯¸í„°ëŠ” í•„ìš”ì— ë§ê²Œ ì¡°ì •)
    llm_for_chunking = Ollama(model="llama2", temperature=0)
    # í•„ìš”í•œ ì¸ìë¥¼ ì „ë‹¬í•˜ì—¬ SemanticChunker ì´ˆê¸°í™” (ì˜ˆ: chunk_size, chunk_overlap ë“±)
    chunker = SemanticChunker(llm=llm_for_chunking, chunk_size=512, chunk_overlap=50)
    
    docs_chunks = []
    docs_metadatas = []
    for doc, meta in zip(documents, metadatas):
        chunks = chunker.split_text(doc)
        docs_chunks.extend(chunks)
        docs_metadatas.extend([meta] * len(chunks))
    
    # ì„ë² ë”© ê³„ì‚° (HuggingFaceEmbeddings ì‚¬ìš©)
    embeddings = HuggingFaceEmbeddings()
    
    # FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    vectorstore = FAISS.from_texts(docs_chunks, embeddings, metadatas=docs_metadatas)
    return vectorstore

def answer_query(query: str, vectorstore, top_k: int = 3) -> str:
    """
    ì…ë ¥ëœ ì§ˆì˜ì— ëŒ€í•´ ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ìœ ì‚¬í•œ ì²­í¬ë“¤ì„ ê²€ìƒ‰í•˜ê³ ,
    ê²€ìƒ‰ëœ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ Ollama LLMì´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    docs = vectorstore.similarity_search(query, k=top_k)
    context = "\n".join([doc.page_content for doc in docs])
    
    # LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = (
        f"ë‹¤ìŒ ë¬¸ë§¥ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:\n\n"
        f"ë¬¸ë§¥:\n{context}\n\n"
        f"ì§ˆë¬¸: {query}\n\n"
        f"ë‹µë³€:"
    )
    
    # Ollama LLM í˜¸ì¶œ (ëª¨ë¸ ì´ë¦„ì€ í™˜ê²½ì— ë§ê²Œ ì¡°ì •)
    llm = Ollama(model="llama2")
    answer = llm(prompt)
    return answer

# RAGìš© ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (ì „ì²´ CSV ë°ì´í„°ë¥¼ ëŒ€ìƒìœ¼ë¡œ)
vectorstore = build_vectorstore(df)

st.markdown("---")
st.header("ğŸ” RAG ì§ˆì˜ ì‘ë‹µ")
rag_query = st.text_input("RAG ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", key="rag_query")
if rag_query:
    response = answer_query(rag_query, vectorstore)
    st.markdown("### ë‹µë³€:")
    st.write(response)
