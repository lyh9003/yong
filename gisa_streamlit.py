import streamlit as st
import pandas as pd
import datetime
import time
import openai
from langchain.vectorstores import FAISS
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA
from langchain.document_loaders import DataFrameLoader
from langchain.text_splitter import CharacterTextSplitter

# OpenAI API í‚¤ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ì§ì ‘ ì…ë ¥ ê°€ëŠ¥)
openai.api_key = st.secrets["OPENAI_API_KEY"]

GITHUB_CSV_URL = f"https://raw.githubusercontent.com/lyh9003/yong/main/Total_Filtered_No_Comment.csv?nocache={int(time.time())}"

def load_data():
    """CSVë¥¼ ë¶ˆëŸ¬ì™€ DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    df = pd.read_csv(GITHUB_CSV_URL, encoding='utf-8-sig')
    df['date'] = pd.to_datetime(df['date'], errors='coerce')
    df = df.sort_values(by='date', ascending=False)
    df['í‚¤ì›Œë“œ_ëª©ë¡'] = df['í‚¤ì›Œë“œ'].apply(lambda x: [k.strip() for k in str(x).split(',')] if pd.notna(x) else [])
    return df

def create_vector_db(df):
    """FAISS ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    df = df.dropna(subset=['title', 'summary'])
    df['content'] = df['title'] + "\n" + df['summary']
    
    # LangChain ë°ì´í„° ë¡œë” ì ìš©
    loader = DataFrameLoader(df, page_content_column='content')
    documents = loader.load()
    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    split_docs = text_splitter.split_documents(documents)
    
    # FAISS ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•
    embeddings = OpenAIEmbeddings()
    vector_db = FAISS.from_documents(split_docs, embeddings)
    return vector_db

def query_rag(vector_db, query):
    """RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê¸°ì‚¬ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    retriever = vector_db.as_retriever(search_kwargs={"k": 3})
    llm = OpenAI(temperature=0.3)
    qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever)
    response = qa_chain.run(query)
    return response

# Streamlit UI ì„¤ì •
df = load_data()
vector_db = create_vector_db(df)

st.title("ğŸ“¢ ë°˜ë„ì²´ ë‰´ìŠ¤ RAG ê¸°ë°˜ ê²€ìƒ‰")
st.write("ë¬¸ì˜/ì•„ì´ë””ì–´ : yh9003.lee@samsung.com")

# ì‚¬ìš©ì ì…ë ¥
query = st.text_input("ğŸ” ê²€ìƒ‰í•  ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”:")
if query:
    with st.spinner("ğŸ” ê²€ìƒ‰ ì¤‘..."):
        response = query_rag(vector_db, query)
    st.markdown("### ğŸ“œ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½")
    st.write(response)

# ê¸°ë³¸ì ì¸ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥
st.write(f"**ì´ ê¸°ì‚¬ ìˆ˜:** {len(df)}ê°œ")
grouped_by_date = df.groupby(df['date'].dt.date, sort=False)
for current_date, date_group in grouped_by_date:
    st.markdown(f"## {current_date.strftime('%Y-%m-%d')}")
    for idx, row in date_group.iterrows():
        with st.expander(f"ğŸ“° {row['title']}"):
            st.write(f"**ìš”ì•½:** {row.get('summary', 'ìš”ì•½ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.')}")
            link = row.get('link', None)
            if pd.notna(link):
                st.markdown(f"[ğŸ”— ê¸°ì‚¬ ë§í¬]({link})")
